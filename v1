import os
import requests
from tkinter import Tk, Text, Button, Label, filedialog, messagebox, Frame
from tkinter.ttk import Notebook
from urllib.parse import urlparse
from bs4 import BeautifulSoup
import docx
import pandas as pd
from difflib import SequenceMatcher
from openpyxl import load_workbook
from openpyxl.styles import PatternFill
from docx.shared import RGBColor
import openpyxl
from requests.packages.urllib3.exceptions import InsecureRequestWarning
import re

# Função para buscar URLs a partir do sitemap
def fetch_urls_from_sitemap(sitemap_url):
    """Fetch URLs from a sitemap."""
    try:
        response = requests.get(sitemap_url, verify=False)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'xml')
        urls = [loc.text for loc in soup.find_all('loc')]
        return urls
    except Exception as e:
        messagebox.showerror("Erro", f"Erro ao processar o sitemap: {e}")
        return []

import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import docx
import pandas as pd

def save_content_to_doc(url, folder):
    """Fetch content from a URL and save to a Word document with enhanced formatting."""
    try:
        response = requests.get(url, verify=False)  # SSL verification disabled
        response.raise_for_status()

        # Generate filename based on the URL
        parsed_url = urlparse(url)
        filename = parsed_url.netloc.replace('.', '_') + parsed_url.path.replace('/', '_')
        if not filename.strip('_'):
            filename = "index"
        filename += ".docx"

        # Parse HTML content
        soup = BeautifulSoup(response.text, 'html.parser')
        doc = docx.Document()
        doc.add_heading(f"Conteúdo de: {url}", level=1)

        # Remove unwanted elements (e.g., navigation, footer)
        for element in soup.find_all(['nav', 'footer', 'aside']):
            element.decompose()

        # Process and format HTML content
        for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'ul', 'ol']):
            if tag.name.startswith('h'):  # Headings
                level = int(tag.name[1])
                doc.add_heading(tag.get_text(strip=True), level=level)
            elif tag.name == 'p':  # Paragraphs
                add_paragraph_with_formatting(doc, tag)
            elif tag.name in ['ul', 'ol']:  # Lists
                add_list_with_formatting(doc, tag)

        # Save the document
        file_path = os.path.join(folder, filename)
        doc.save(file_path)
        return file_path
    except Exception as e:
        return str(e)

def add_paragraph_with_formatting(doc, tag):
    """Add a paragraph to the Word document, preserving bold, italic, and hyperlinks."""
    para = doc.add_paragraph()
    for part in tag.contents:
        if isinstance(part, str):  # Plain text
            para.add_run(part)
        elif part.name == 'a':  # Hyperlinks
            link_text = part.get_text(strip=True)
            link_url = part.get('href', '')
            
            if para.text:  # Check if there is existing text, add a space before the link
                para.add_run(" ")  # Add a space before the link
            
            run = para.add_run(link_text)
            run.font.underline = True
            para.add_run(f" ({link_url})")
            
            # After the link, we ensure that there is a space before continuing with other content.
            para.add_run(" ")  # Ensure space after the link text
        
        elif part.name in ['strong', 'b']:  # Bold text
            run = para.add_run(part.get_text(strip=True))
            run.bold = True
        elif part.name in ['em', 'i']:  # Italics
            run = para.add_run(part.get_text(strip=True))
            run.italic = True
        else:  # Other tags or plain text
            para.add_run(part.get_text(strip=True) if part else '')

    # Ensure proper spacing after bold and italicized text
    para_format = para.paragraph_format
    para_format.space_after = docx.shared.Pt(6)  # Adjust space after paragraph if needed

def add_list_with_formatting(doc, tag):
    """Add a list (ordered or unordered) to the Word document."""
    list_style = 'ListBullet' if tag.name == 'ul' else 'ListNumber'
    for li in tag.find_all('li'):
        para = doc.add_paragraph(style=list_style)
        for part in li.contents:
            if isinstance(part, str):  # Plain text
                para.add_run(part)
            elif part.name == 'a':  # Hyperlinks
                link_text = part.get_text(strip=True)
                link_url = part.get('href', '')
                run = para.add_run(link_text)
                run.font.underline = True
                para.add_run(f" ({link_url})")
            elif part.name in ['strong', 'b']:  # Bold text
                run = para.add_run(part.get_text(strip=True))
                run.bold = True
            elif part.name in ['em', 'i']:  # Italics
                run = para.add_run(part.get_text(strip=True))
                run.italic = True
            else:  # Other tags or plain text
                para.add_run(part.get_text(strip=True) if part else '')

def save_content_to_excel(urls, folder):
    """Fetch content from URLs and save to an Excel file."""
    data = []
    for url in urls:
        try:
            response = requests.get(url, verify=False)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            text_content = "\n".join([tag.get_text(strip=True) for tag in soup.find_all(['h1', 'h2', 'h3', 'p'])])
            data.append({"URL": url, "Conteúdo": text_content})
        except Exception as e:
            data.append({"URL": url, "Conteúdo": f"Erro: {e}"})

    file_path = os.path.join(folder, "conteudo_urls.xlsx")
    df = pd.DataFrame(data)
    df.to_excel(file_path, index=False)
    return file_path

def process_urls(urls, output_dir, save_as_doc):
    """Process and save content for each URL."""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    if save_as_doc:
        for url in urls:
            url = url.strip()
            if url:
                status = save_content_to_doc(url, output_dir)
                print(f"Processed {url}: {status}")
    else:
        file_path = save_content_to_excel(urls, output_dir)
        print(f"Saved content to Excel: {file_path}")


# Função para iniciar o scraper
def start_scraper():
    """Start the scraper with input URLs or sitemap."""
    input_text = url_input.get("1.0", "end").strip()
    urls = []

    if input_text.startswith("http") and input_text.endswith(".xml"):
        urls = fetch_urls_from_sitemap(input_text)
    else:
        urls = [line.strip() for line in input_text.splitlines() if line.strip()]

    if not urls:
        messagebox.showwarning("Aviso", "Nenhuma URL fornecida ou válida encontrada.")
        return

    output_dir = filedialog.askdirectory(title="Selecione o diretório de saída")
    if not output_dir:
        return

    save_as_doc = messagebox.askyesno("Salvar como", "Deseja salvar como documentos Word (.docx)? (Selecionar 'Não' salvará como Excel)")
    process_urls(urls, output_dir, save_as_doc)
    messagebox.showinfo("Concluído", f"Conteúdo salvo na pasta: {output_dir}")

# Função para avaliar se o conteúdo de um .docx está presente em uma URL
def evaluate_doc_against_url():
    """Evaluate if the content of a .docx is fully present in a URL."""
    doc_path = filedialog.askopenfilename(title="Selecione o arquivo .docx", filetypes=[("Documentos Word", "*.docx")])
    if not doc_path:
        return

    url = url_entry.get("1.0", "end").strip()
    if not url:
        messagebox.showwarning("Aviso", "Por favor, insira uma URL para avaliação.")
        return

    try:
        # Extract text from the .docx file
        doc = docx.Document(doc_path)
        doc_text_lines = [p.text.strip() for p in doc.paragraphs if p.text.strip()]

        # Fetch text from the URL
        response = requests.get(url, verify=False)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        # Process page text, including headings, paragraphs, and lists
        page_text_lines = []
        # Collect text from headings
        for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
            page_text_lines.append(tag.get_text(strip=True))
        # Collect text from paragraphs
        for tag in soup.find_all('p'):
            page_text_lines.append(tag.get_text(" ", strip=True))  # Preserve spaces between words
        # Collect text from lists (unordered and ordered)
        for tag in soup.find_all(['ul', 'ol']):
            for li in tag.find_all('li'):
                page_text_lines.append(li.get_text(" ", strip=True))  # Preserve spaces between list items

        # Now handle <a> tags (links) separately to preserve spacing around them
        for a_tag in soup.find_all('a'):
            link_text = a_tag.get_text(strip=True)  # Only collect the text inside the <a> tag
            page_text_lines.append(link_text)  # Don't include extra spaces for links

        # Now process the content for URLs directly in text
        url_pattern = r'(https?://[^\s]+)'  # Regex to match URLs
        for line in doc_text_lines:
            # Replace URLs with space-preserving versions, keeping them intact
            line = re.sub(url_pattern, lambda match: match.group(0), line)  # Retain URL exactly as is
            page_text_lines.append(line)  # Add it to the page text for comparison

        # Prepare the comparison results
        results = []
        for doc_line in doc_text_lines:
            # Remove extra spaces from both document and page text before comparison
            doc_line_cleaned = ' '.join(doc_line.split())  # Normalize spaces in doc line
            best_match = max(page_text_lines, key=lambda x: SequenceMatcher(None, doc_line_cleaned, x).ratio())
            best_match_cleaned = ' '.join(best_match.split())  # Normalize spaces in page line

            # Calculate similarity
            similarity = SequenceMatcher(None, doc_line_cleaned, best_match_cleaned).ratio()

            if similarity == 1.0:
                status = "Presente"
            elif similarity >= 0.4:
                status = "Semelhante"
            else:
                status = "Ausente"

            results.append({
                "Texto do Doc": doc_line,
                "Texto da Página": best_match if similarity > 0 else "N/A",
                "Status": status
            })

        # Seleção da pasta de saída
        output_file = filedialog.asksaveasfilename(defaultextension=".xlsx", filetypes=[("Excel Files", "*.xlsx")], title="Salvar Resultado de Avaliação")
        if not output_file:
            return

        # Save the results to an Excel file
        df = pd.DataFrame(results)
        df.to_excel(output_file, index=False)

        # Load the Excel file for styling
        wb = load_workbook(output_file)
        ws = wb.active

        # Define color fills
        green_fill = PatternFill(start_color="00FF00", end_color="00FF00", fill_type="solid")
        yellow_fill = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")
        red_fill = PatternFill(start_color="FF0000", end_color="FF0000", fill_type="solid")

        # Apply colors to the "Status" column based on the value
        for row in range(2, len(results) + 2):  # Starting from row 2 to avoid the header
            status_cell = ws.cell(row=row, column=3)
            if status_cell.value == "Presente":
                status_cell.fill = green_fill
            elif status_cell.value == "Semelhante":
                status_cell.fill = yellow_fill
            elif status_cell.value == "Ausente":
                status_cell.fill = red_fill

        # Save the styled Excel file
        wb.save(output_file)

        messagebox.showinfo("Concluído", f"Avaliação concluída! Arquivo salvo em:\n{output_file}")
    except Exception as e:
        messagebox.showerror("Erro", f"Erro ao avaliar o conteúdo: {e}")
# Configuração da UI
root = Tk()
root.title("Web Scraper e Avaliador")
root.geometry("600x500")

notebook = Notebook(root)
notebook.pack(fill="both", expand=True)

# Aba 1: Web Scraper
scraper_tab = Frame(notebook)
notebook.add(scraper_tab, text="Web Scraper")

Label(scraper_tab, text="Insira URLs (uma por linha) ou o URL de um sitemap:").pack(pady=10)
url_input = Text(scraper_tab, height=15, width=70)
url_input.pack(pady=10)
Button(scraper_tab, text="Iniciar Scraper", command=start_scraper, bg="purple", fg="white").pack(pady=10)

# Aba 2: Avaliador de Conteúdo
evaluator_frame = Frame(notebook)
notebook.add(evaluator_frame, text="Avaliador de Conteúdo")

Label(evaluator_frame, text="Insira a URL para avaliação:").pack(pady=10)
url_entry = Text(evaluator_frame, height=1, width=70)
url_entry.pack(pady=10)
Button(evaluator_frame, text="Iniciar Avaliação", command=evaluate_doc_against_url, bg="purple", fg="white").pack(pady=10)

root.mainloop()
