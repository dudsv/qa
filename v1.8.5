import os
import requests
from tkinter import Tk, Text, Button, Label, filedialog, messagebox, Frame, StringVar
from tkinter.ttk import Notebook, Style, Button
from urllib.parse import urlparse
from bs4 import BeautifulSoup
import docx
import pandas as pd
from difflib import SequenceMatcher
from openpyxl import load_workbook
from openpyxl.styles import PatternFill
from openpyxl.styles import Alignment

# Função para buscar URLs a partir do sitemap
def fetch_urls_from_sitemap(sitemap_url):
    """Fetch URLs from a sitemap."""
    try:
        response = requests.get(sitemap_url, verify=False)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'xml')
        urls = [loc.text for loc in soup.find_all('loc')]
        return urls
    except Exception as e:
        messagebox.showerror("Erro", f"Erro ao processar o sitemap: {e}")
        return []

# Função para extrair textos dos accordions
from bs4 import BeautifulSoup

from bs4 import BeautifulSoup

def extract_accordion_texts(html_content, 
                            accordion_selectors=None, 
                            title_selectors=None, 
                            body_selectors=None, 
                            strip_html=True, 
                            log_errors=False):
    soup = BeautifulSoup(html_content, 'html.parser')
    accordion_texts = []
    
    accordion_selectors = accordion_selectors or [
        '.paragraph--type--accordion',
        '[class*="accordion"]',
        '.paragraph--type--accordion-item',
        '.accordion-item',
        '.field--name-field-c-text'
    ]
    
    title_selectors = title_selectors or [
        'button.accordion-button',
        'h2 button',
        'h2[id^="acc-head"]',
        'button[data-toggle="collapse"]',
        'button[data-target]',
        '[aria-controls]',
        '.accordion-header'
    ]
    
    body_selectors = body_selectors or [
        '.accordion-collapse .field__item',
        '.accordion-body',
        '.field__item .text-formatted',
        'div[id^="acc-body"]',
        '.paragraph--type--c-text .field__item'
    ]
    
    for container_selector in accordion_selectors:
        containers = soup.select(container_selector)
        
        for container in containers:
            # Skip nested accordion items
            if container.find_parent(container_selector):
                continue
            
            # Find title
            title = None
            for title_selector in title_selectors:
                title_element = container.select_one(title_selector)
                if title_element:
                    title = title_element.get_text(strip=True)
                    break
            
            # Find body
            body = None
            for body_selector in body_selectors:
                body_element = container.select_one(body_selector)
                if body_element:
                    body = body_element.get_text(strip=True) if strip_html else str(body_element)
                    break
            
            # Log errors for debugging
            if log_errors and (not title or not body):
                print(f"Missing {'title' if not title else 'body'} in container: {container}")
            
            # Add to result if both title and body are found and not duplicate
            if title and body and not any(item['title'] == title for item in accordion_texts):
                accordion_texts.append({
                    'title': title,
                    'body': body
                })
    
    return accordion_texts

def save_content_to_doc(url, folder):
    """Fetch content from a URL and save to a Word document with enhanced formatting."""
    try:
        response = requests.get(url, verify=False) # SSL verification disabled
        response.raise_for_status()
        # Generate filename based on the URL
        parsed_url = urlparse(url)
        filename = parsed_url.netloc.replace('.', '_') + parsed_url.path.replace('/', '_')
        if not filename.strip('_'):
            filename = "index"
        filename += ".docx"
        # Parse HTML content
        soup = BeautifulSoup(response.text, 'html.parser')
        doc = docx.Document()
        doc.add_heading(f"Conteúdo de: {url}", level=1)
        # Remove unwanted elements (e.g., navigation, footer)
        for element in soup.find_all(['nav', 'footer', 'aside']):
            element.decompose()
        # Process and format HTML content
        for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'ul', 'ol']):
            if tag.name.startswith('h'): # Headings
                level = int(tag.name[1])
                doc.add_heading(tag.get_text(strip=True), level=level)
            elif tag.name == 'p': # Paragraphs
                add_paragraph_with_formatting(doc, tag)
            elif tag.name in ['ul', 'ol']: # Lists
                add_list_with_formatting(doc, tag)
        # Save the document
        file_path = os.path.join(folder, filename)
        doc.save(file_path)
        return file_path
    except Exception as e:
        return str(e)

def add_paragraph_with_formatting(doc, tag):
    """Adiciona um parágrafo no documento Word, preservando negrito, itálico e hyperlinks."""
    para = doc.add_paragraph()
    for i, part in enumerate(tag.contents):
        if isinstance(part, str): # Texto simples
            para.add_run(part)
        elif part.name == 'a': # Links
            link_text = part.get_text(strip=True)
            link_url = part.get('href', '')
            
            if para.text: # Se já houver texto no parágrafo, adiciona um espaço antes do link
                para.add_run(" ") # Adiciona um espaço antes do link
            
            run = para.add_run(link_text)
            run.font.underline = True
            para.add_run(f" ({link_url})")
            
            # Após o link, garante um espaço para o próximo conteúdo
            para.add_run(" ") # Garante espaço após o link
        elif part.name in ['strong', 'b']: # Texto em negrito
            run = para.add_run(part.get_text(strip=True))
            run.bold = True
        elif part.name in ['em', 'i']: # Texto em itálico
            run = para.add_run(part.get_text(strip=True))
            run.italic = True
        else: # Outros elementos ou texto simples
            para.add_run(part.get_text(strip=True) if part else '')
        
        # Se não for o último item e a parte não for um link, adiciona um espaço
        if i < len(tag.contents) - 1 and not isinstance(part, str) and part.name != 'a':
            para.add_run(" ") # Adiciona um espaço entre os elementos
    
    # Ajuste no espaçamento após parágrafos com formatação
    para_format = para.paragraph_format
    para_format.space_after = docx.shared.Pt(6) # Ajuste opcional do espaçamento após o parágrafo

def add_list_with_formatting(doc, tag):
    """Add a list (ordered or unordered) to the Word document."""
    list_style = 'ListBullet' if tag.name == 'ul' else 'ListNumber'
    for li in tag.find_all('li'):
        para = doc.add_paragraph(style=list_style)
        for part in li.contents:
            if isinstance(part, str): # Plain text
                para.add_run(part)
            elif part.name == 'a': # Hyperlinks
                link_text = part.get_text(strip=True)
                link_url = part.get('href', '')
                run = para.add_run(link_text)
                run.font.underline = True
                para.add_run(f" ({link_url})")
            elif part.name in ['strong', 'b']: # Bold text
                run = para.add_run(part.get_text(strip=True))
                run.bold = True
            elif part.name in ['em', 'i']: # Italics
                run = para.add_run(part.get_text(strip=True))
                run.italic = True
            else: # Other tags or plain text
                para.add_run(part.get_text(strip=True) if part else '')

def save_content_to_excel(urls, folder):
    """Fetch content from URLs and save to an Excel file."""
    data = []
    for url in urls:
        try:
            response = requests.get(url, verify=False)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            text_content = "\n".join([tag.get_text(strip=True) for tag in soup.find_all(['h1', 'h2', 'h3', 'p'])])
            data.append({"URL": url, "Conteúdo": text_content})
        except Exception as e:
            data.append({"URL": url, "Conteúdo": f"Erro: {e}"})
    file_path = os.path.join(folder, "conteudo_urls.xlsx")
    df = pd.DataFrame(data)
    df.to_excel(file_path, index=False)
    return file_path

def process_urls(urls, output_dir, save_as_doc):
    """Process and save content for each URL."""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    if save_as_doc:
        for url in urls:
            url = url.strip()
            if url:
                status = save_content_to_doc(url, output_dir)
                print(f"Processed {url}: {status}")
    else:
        file_path = save_content_to_excel(urls, output_dir)
        print(f"Saved content to Excel: {file_path}")

# Função para iniciar o scraper
def start_scraper():
    """Start the scraper with input URLs or sitemap."""
    input_text = url_input.get("1.0", "end").strip()
    urls = []
    if input_text.startswith("http") and input_text.endswith(".xml"):
        urls = fetch_urls_from_sitemap(input_text)
    else:
        urls = [line.strip() for line in input_text.splitlines() if line.strip()]
    if not urls:
        messagebox.showwarning("Aviso", "Nenhuma URL fornecida ou válida encontrada.")
        return
    output_dir = filedialog.askdirectory(title="Selecione o diretório de saída")
    if not output_dir:
        return
    save_as_doc = messagebox.askyesno("Salvar como", "Deseja salvar como documentos Word (.docx)? (Selecionar 'Não' salvará como Excel)")
    process_urls(urls, output_dir, save_as_doc)
    messagebox.showinfo("Concluído", f"Conteúdo salvo na pasta: {output_dir}")

# Função para avaliar se o conteúdo de um .docx está presente em uma URL
def evaluate_doc_against_url():
    """Evaluate if the content of a .docx is fully present in a URL and export the results to Excel."""
    import re
    from difflib import SequenceMatcher
    from tkinter import filedialog, messagebox
    import docx
    import requests
    from bs4 import BeautifulSoup
    import pandas as pd
    from openpyxl import load_workbook
    from openpyxl.styles import PatternFill

    def clean_text(text):
        """Remove URLs and clean up text for comparison."""
        # 1. Remove internal links with text in parentheses
        text = re.sub(r'\s*\([^()]*?(?:http|www|/)[^()]*?\)\s*', ' ', text)
        # 2. URLs in parentheses with space before/after
        text = re.sub(r'\s*\([^()]*?(?:http|www|/)[^()]*?\)\s*', ' ', text)
        # 3. URLs in parentheses without space
        text = re.sub(r'\([^()]*?(?:http|www|/)[^()]*?\)', '', text)
        # 4. Standalone URLs
        text = re.sub(r'(?:http|www)[^\s\(\)]+', '', text)
        # 5. URLs with surrounding text in parentheses
        text = re.sub(r'\([^()]*?(?:http|www|/)[^()]*?\)', '', text)
        # 6. Remove multiple spaces and normalize
        text = re.sub(r'\s+', ' ', text)
        # 7. Remove empty parentheses that might remain
        text = re.sub(r'\(\s*\)', '', text)
        # 8. Clean up any remaining artifacts
        text = re.sub(r'\s+', ' ', text)
        # 9. Remove spaces before punctuation
        text = re.sub(r'\s+([.,!?:])', r'\1', text)
        return text.strip()

    def extract_highlighted_words(soup):
        """Extract highlighted words (bold, italic, headings) and hyperlinks from the HTML content, ignoring nav, footer, and menus."""
        highlighted = {
            'Bold': [],
            'Italic': [],
            'Headings': [],
            'Hyperlinks': []
        }
        for tag in soup.find_all(['strong', 'b', 'em', 'i', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'a']):
            # Ignore elements within nav, footer, and menus
            if tag.find_parents(['nav', 'footer', 'aside']):
                continue
            text = tag.get_text(strip=True)
            if tag.name in ['strong', 'b']:
                highlighted['Bold'].append(text)
            elif tag.name in ['em', 'i']:
                highlighted['Italic'].append(text)
            elif tag.name.startswith('h'):
                highlighted['Headings'].append(f"{tag.name.upper()}: {text}")
            elif tag.name == 'a':
                href = tag.get('href', '')
                highlighted['Hyperlinks'].append(f"{text} ({href})")
        return highlighted

    # Select the Word document
    doc_path = filedialog.askopenfilename(title="Selecione o arquivo .docx", filetypes=[("Documentos Word", "*.docx")])
    if not doc_path:
        return
    # Get the URL from the user interface
    url = url_entry.get("1.0", "end").strip()
    if not url:
        messagebox.showwarning("Aviso", "Por favor, insira uma URL para avaliação.")
        return
    try:
        # Extract text from the .docx file
        doc = docx.Document(doc_path)
        # Clean and store original text for display
        doc_text_lines_original = [p.text.strip() for p in doc.paragraphs if p.text.strip()]
        # Clean text for comparison
        doc_text_lines = [clean_text(p.text.strip()) for p in doc.paragraphs if p.text.strip()]
        # Fetch text from the URL
        response = requests.get(url, verify=False)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        # Process page text, including headings, paragraphs, and lists
        page_text_lines = []
        def process_text_block(element):
         
            """Process text content with improved extraction and preservation of context."""
            text_parts = []
            for content in element.contents:
                current_text = ""
                if isinstance(content, str):
                    current_text = content.strip()
                elif content.name in ['strong', 'b', 'em', 'i', 'a', 'span', 'p', 'ul', 'ol', 'li', 'div']:
                    current_text = content.get_text(strip=True)
                
                if current_text:
                    text_parts.append(current_text)
            
            return ' '.join(text_parts).strip()
        # Primeiro, encontre todas as tags de texto relevantes
        text_elements = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'li', 'div'])
        # Processo cada elemento encontrado
        for element in text_elements:
            # Ignore divs que são containers ou têm classes específicas de navegação/rodapé
            if element.name == 'div':
                classes = element.get('class', [])
                if any(c in ' '.join(classes).lower() for c in ['nav', 'footer', 'header', 'menu']):
                    continue
            # Para divs, só processe se tiver texto direto (não apenas texto de children)
            if not element.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'li']):
                processed_text = process_text_block(element)
                if processed_text:
                    page_text_lines.append(processed_text)
            else:
                # Para outros elementos, processe normalmente
                processed_text = process_text_block(element)
                if processed_text:
                    page_text_lines.append(processed_text)
        # Clean page text for comparison
        page_text_lines = [clean_text(line) for line in page_text_lines if line.strip()]
        # Remove duplicates mantendo a ordem
        page_text_lines = list(dict.fromkeys(page_text_lines))
        # Extract highlighted words and hyperlinks
        highlighted_words = extract_highlighted_words(soup)
        # Prepare the comparison results
        results = []
        for i, doc_line in enumerate(doc_text_lines):
            # Find the best match between document text and page text
            best_match = max(page_text_lines, key=lambda x: SequenceMatcher(None, doc_line, x).ratio())
            similarity = SequenceMatcher(None, doc_line, best_match).ratio()
            if similarity == 1.0:
                status = "Presente"
            elif similarity >= 0.7:
                status = "Semelhante"
            else:
                status = "Ausente"
            results.append({
                "Texto do Doc": doc_text_lines_original[i], # Use original text for display
                "Texto da Página": best_match if similarity > 0 else "N/A",
                "Status": status,
                "Similaridade": f"{similarity:.2%}" # Adicionado percentual de similaridade
            })
        # Convert results to a DataFrame
        df = pd.DataFrame(results)
        # Save results to an Excel file
        save_path = filedialog.asksaveasfilename(
            title="Salvar resultados", defaultextension=".xlsx",
            filetypes=[("Planilhas Excel", "*.xlsx")]
        )
        if save_path:
            # First save with pandas
            df.to_excel(save_path, index=False)
            # Then load for styling and adding new sheet
            wb = load_workbook(save_path)
            ws = wb.active
            # Define color fills
            green_fill = PatternFill(start_color="92D050", end_color="92D050", fill_type="solid") # Softer green
            yellow_fill = PatternFill(start_color="FFEB9C", end_color="FFEB9C", fill_type="solid") # Softer yellow
            red_fill = PatternFill(start_color="FF9999", end_color="FF9999", fill_type="solid") # Softer red
            # Apply colors to the "Status" column based on the value
            for row in range(2, len(results) + 2): # Starting from row 2 to avoid the header
                status_cell = ws.cell(row=row, column=3) # Status is in column 3
                if status_cell.value == "Presente":
                    status_cell.fill = green_fill
                elif status_cell.value == "Semelhante":
                    status_cell.fill = yellow_fill
                elif status_cell.value == "Ausente":
                    status_cell.fill = red_fill
            
            # Enhanced column width and text wrapping
            for column in ws.columns:
                max_word_length = 0
                column_letter = column[0].column_letter
                
                for cell in column:
                    try:
                        # Check for extremely long words
                        longest_word = max(str(cell.value).split(), key=len) if cell.value else ''
                        max_word_length = max(max_word_length, len(longest_word))
                        
                        # Enable text wrapping
                        cell.alignment = openpyxl.styles.Alignment(wrap_text=True, vertical='top')
                    except Exception:
                        pass
                
                # Adjust column width based on longest word, but cap it
                adjusted_width = min(max_word_length + 5, 50) # Limit max width to 50
                ws.column_dimensions[column_letter].width = adjusted_width
            # Create a new sheet for highlighted words
            ws_highlighted = wb.create_sheet(title="Palavras Destacadas")
            ws_highlighted.append(["Tipo", "Palavras"])
            for key, words in highlighted_words.items():
                for word in words:
                    ws_highlighted.append([key, word])
            # Save the styled Excel file
            wb.save(save_path)
            messagebox.showinfo("Sucesso", f"Os resultados foram salvos em {save_path}")
    except requests.RequestException as e:
        messagebox.showerror("Erro", f"Erro ao acessar a URL: {e}")
    except Exception as e:
        messagebox.showerror("Erro", f"Ocorreu um erro inesperado: {e}")

# Updated UI Configuration
def create_rounded_dark_theme(root):
    """Create a rounded, dark-themed UI with purple accent."""
    root.configure(bg='#1E1E2E') # Deep dark background
    root.option_add('*Background', '#1E1E2E') # Global background
    root.option_add('*Foreground', '#E0E0E0') # Light text
    root.option_add('*Font', ('Helvetica', 10)) # Clean, modern font
    style = Style()
    style.theme_use('clam') # More customizable theme
    # Notebook style
    style.configure('TNotebook', background='#1E1E2E', borderwidth=0)
    style.configure('TNotebook.Tab', 
                    background='#2C2C3E', 
                    foreground='#E0E0E0', 
                    padding=[10, 5], 
                    font=('Helvetica', 10, 'bold')
                   )
    style.map('TNotebook.Tab', 
              background=[('selected', '#6A5ACD')], # Selected tab color
              foreground=[('selected', 'white')]
             )
    # Button style
    style.configure('TButton', 
                    background='#6A5ACD', # Purple accent
                    foreground='white',
                    font=('Helvetica', 10, 'bold'),
                    borderwidth=0,
                    relief='flat'
                   )
    style.map('TButton', 
              background=[('active', '#7B68EE')], # Lighter purple on hover
              foreground=[('active', 'white')]
             )
    return style

def start_scraper():
    """Start the scraper with enhanced error handling and UI feedback."""
    input_text = url_input.get("1.0", "end").strip()
    urls = []
    if input_text.startswith("http") and input_text.endswith(".xml"):
        urls = fetch_urls_from_sitemap(input_text)
    else:
        urls = [line.strip() for line in input_text.splitlines() if line.strip()]
    
    if not urls:
        messagebox.showwarning("Aviso", "Nenhuma URL fornecida ou válida encontrada.")
        return
    
    output_dir = filedialog.askdirectory(title="Selecione o diretório de saída")
    if not output_dir:
        return
    
    save_as_doc = messagebox.askyesno("Salvar como", "Deseja salvar como documentos Word (.docx)? (Selecionar 'Não' salvará como Excel)")
    process_urls(urls, output_dir, save_as_doc)
    messagebox.showinfo("Concluído", f"Conteúdo salvo na pasta: {output_dir}")

# Main Application Setup
root = Tk()
root.title("Web Scraper e Avaliador")
root.geometry("700x600")
root.configure(bg='#1E1E2E')
# Create dark theme
theme_style = create_rounded_dark_theme(root)
# Notebook with rounded look
notebook = Notebook(root, style='TNotebook')
notebook.pack(fill="both", expand=True, padx=15, pady=15, ipadx=10, ipady=10)
# Scraper Tab
scraper_tab = Frame(notebook, bg='#1E1E2E', highlightthickness=2, highlightcolor='#6A5ACD', highlightbackground='#6A5ACD')
notebook.add(scraper_tab, text="Web Scraper")
# Custom rounded text widget
url_input = Text(
    scraper_tab, 
    height=15, 
    width=70, 
    bg='#2C2C3E', 
    fg='#E0E0E0', 
    insertbackground='white',
    selectbackground='#6A5ACD',
    relief='flat',
    borderwidth=10,
    highlightthickness=2,
    highlightcolor='#6A5ACD',
    highlightbackground='#6A5ACD'
)
url_input.pack(pady=10)
Label(
    scraper_tab, 
    text="Insira URLs (uma por linha) ou o URL de um sitemap:", 
    bg='#1E1E2E', 
    fg='#E0E0E0', 
    font=('Helvetica', 12)
).pack(pady=5)
# Rounded Button
scraper_button = Button(
    scraper_tab, 
    text="Iniciar Scraper", 
    command=start_scraper,
    style='Rounded.TButton'
)
scraper_button.pack(pady=10)
# Evaluator Tab
evaluator_frame = Frame(notebook, bg='#1E1E2E', highlightthickness=2, highlightcolor='#6A5ACD', highlightbackground='#6A5ACD')
notebook.add(evaluator_frame, text="Avaliador de Conteúdo")
url_entry = Text(
    evaluator_frame, 
    height=1, 
    width=70, 
    bg='#2C2C3E', 
    fg='#E0E0E0', 
    insertbackground='white',
    selectbackground='#6A5ACD',
    relief='flat',
    borderwidth=10,
    highlightthickness=2,
    highlightcolor='#6A5ACD',
    highlightbackground='#6A5ACD'
)
url_entry.pack(pady=10)
Label(
    evaluator_frame, 
    text="Insira a URL para avaliação:", 
    bg='#1E1E2E', 
    fg='#E0E0E0', 
    font=('Helvetica', 12)
).pack(pady=5)
# Rounded Button
eval_button = Button(
    evaluator_frame, 
    text="Iniciar Avaliação", 
    command=evaluate_doc_against_url,
    style='Rounded.TButton'
)
eval_button.pack(pady=10)
root.mainloop()
